[project]
name = "attribution"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.0.1",
    "ai2-olmo>=0.2.2",
    "bigcode-eval",
    "datasets>=3.1.0",
    "dolma>=1.0.8",
    "editables>=0.5",
    "evalplus>=0.3.1",
    "fastai>=2.7.18",
    "gpustat>=1.1.1",
    "guidance>=0.2.0",
    "hatchling>=1.26.3",
    "hypothesis>=6.125.3",
    "jupyterlab>=4.2.5",
    "llama-stack>=0.0.49",
    "lovely-tensors>=0.1.17",
    "matplotlib>=3.9.2",
    "nnsight>=0.3.6",
    "numpy>=2.1.2",
    "openai>=1.59.8",
    "plotly>=6.0.0",
    "polars>=1.21.0",
    "ruff>=0.7.2",
    "seaborn>=0.13.2",
    "torch>=2.5.1",
    "torchtyping>=0.1.5",
    "transformers>=4.46.1",
    "vllm>=0.1.2",
]


[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "uv>=0.4.27",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/attribution"]

[tool.uv.sources]
bigcode-eval = { path = "/share/u/lofty/code_llm/bigcode-evaluation-harness", editable = true }
